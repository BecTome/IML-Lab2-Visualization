# Questions and answers

## 1. Is PCA giving you more advice for knowing the underlying information in the data set?

PCA (Principal Component Analysis) is a dimensionality reduction technique that transforms the original features into a new set of orthogonal features, called principal components. These components capture the maximum variance in the data. The analysis provided indicates that PCA has been successful in capturing the underlying information in the datasets. The projection of data into the principal component space allows for visualization, clustering, and exploration of the dataset's structure. The loadings of the principal components provide insights into the contribution of each original feature to the new components, aiding in understanding the dataset's variability.

In this case, for **vote dataset**, it provides useful information about the correlation between "yes" and "no" features, marking as non-informative the results of one of them. In addition to that, original dataset has 16 features, and PCA allows to reduce it to 1 single feature preserving half of the information (47% of total variance). This mitigates the effect of the curse of dimensionality, and allows to use simpler models.

For **glass dataset**, PCA isn't that effective, as variance is not completely captured by the first component. In this case, PCA allows to reduce the dimensionality of the dataset to 2 features, preserving 43% of the information. This is not a big reduction, but it allows to visualize the data in a 2D space, and to use simpler models. However, it can be oserved that, given the complexity of the problem, this dimensionality reduction is not enough to make the problem easily separable.

Finally, in the case of **heart-h dataset**, most of the variance can be explained by the first variable. However, when projected into a 2D space, it can be observed that the data is almost linearly separable. This means that PCA simplifies a lot the problem, reducing computational time and avoiding noise coming from redundant features.

## 2. Is BIRCH giving you more advice for knowing the underlying information in the data set?

The analysis suggests that BIRCH is effective in clustering the data but may not provide as much insight into the underlying information as PCA. While BIRCH is powerful for clustering large datasets and providing a hierarchical structure, it might not offer the same interpretability as PCA in terms of understanding the contribution of each feature to the dataset's structure.

## 3. Can you explain the setup that you have used for PCA algorithm?

The setup for PCA involves applying the PCA algorithm to three datasets: Vote, Glass, and Heart-h. The implementation includes computing the covariance matrix, obtaining the eigenvectors (loadings), projecting the data into the new principal component space, and analyzing the results. The PCA algorithm has been implemented both manually and using the scikit-learn library. The manual implementation involves:

- centering the data (not standardizing)
- computing the covariance matrix, obtaining the eigenvectors 
- projecting the data onto the principal components
- saving eigenvalues as explained variance
- calculate relative explained variance for each of the components

## 4. Can you reduce the dimensionality of the data set? In case of an affirmative answer, detail how do you do and how many features have been reduced from the original data set.

The answer to this question can be found in Question 1.

## 5. Can you detail how you decided to make the reduction using truncatedSVD have you obtained and how much have been reduced from the original data set.

The results after applying SVD were almost identical to the ones obtained by PCA. This is because PCA is a special case of SVD, where the data is centered and scaled. For **glass** and **heart-h** datasets, data is unintentionally centered.

The most particular case was found for **vote dataset**. For this one, singular values weren't sorted in descending order, a first singular value much lower than the rest appeared. This was observed to be due to the scale and the presence of outliers. However, this problem got solved when data was scaled, obtaining the same results as PCA.

## 6. Do you obtain the same results from your code of PCA to the code in sklearn? Explain the similarities and the differences among the two implementations.

They worked mostly in the same way in terms of the results obtained. The explained variance of all the components are equal in both cases and the absolute value of the components.

An observed detail is that in some of the PCA features, the signs are the opposite than the ones provided by sklearn. This is because, mathematically, there is freedom in the choice of the signs. This is consequence of eigenvectors being defined as directions which can be generated by two opposite vectors, both with norm one. This is not a problem, given that orthogonality and unit norm are preserved.

In execution time, the manual implementation resulted slightly slower than the one provided by sklearn. This is because the implementation provided by sklearn is optimized, and it uses a different algorithm to compute the covariance matrix.

## 7. Can you explain if you obtain similar clusters with the data set reduced to those obtained in the original data? In addition, have you observed a reduction in time?

Yes, clustering task performs differently with and without dimensionality reduction for all of hte datasets. The clustering obtained after preserving just the first component outperforms the direct application of KMeans and BIRCH on the original dataset. It does it in terms of execution time and in terms of chosen metrics (silhouette score and V-measure).

## 8. Which are the similarities and differences in the visualizations obtained using PCA and ISOMAP? 

Isomap clustering is a dimensionality reduction and clustering algorithm that can be used to visualize and cluster high-dimensional data. It works by constructing a graph of the data points, where the edges of the graph represent the distances between the data points. The algorithm then finds a lower-dimensional embedding of the data points, while preserving the distances between the data points as much as possible.

In this case, both algorithms provide similar information. This can be due to the fact that the data is not very complex and hence, the linear projection is enough to visualize the data. It's interesting to notice thet internal structure analyzed by SOM algorithm is almost identical than the one found by PCA.

It's clear that, for glass dataset,  the SOM algorithm is able to find a more complex structure than the one found by PCA. This is because the SOM algorithm is able to find nonlinear relationships between the features. This makes the categories much more separable.

Finally, for heart-h a more complex structure is found as in the case of glass. However, in this case, it's not as easy for the algorithms to find separable structures in data.
